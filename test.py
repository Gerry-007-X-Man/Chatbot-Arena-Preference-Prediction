import os
import pandas as pd
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# 1. åŸºç¡€é…ç½®
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
model_path = "./final_chatbot_model"  # ä½ åˆšåˆšä¿å­˜æ¨¡å‹çš„è·¯å¾„

def preprocess_test_function(examples, tokenizer):
    # ä¸è®­ç»ƒæ—¶ä¿æŒå®Œå…¨ä¸€è‡´çš„æ‹¼æ¥é€»è¾‘
    text_pairs = [
        str(a) + " [SEP] " + str(b) 
        for a, b in zip(examples['response_a'], examples['response_b'])
    ]
    
    return tokenizer(
        examples['prompt'],
        text_pair=text_pairs,
        truncation=True,
        max_length=512,
        padding="max_length",
    )

if __name__ == '__main__':
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    test_path = 'ä½ çš„æ–‡ä»¶ç›®å½•/test.csv'
    test_df = pd.read_csv(test_path)
    
    # è®°å½• ID ç”¨äºæœ€åæäº¤
    test_ids = test_df['id'].astype(str).tolist()

    # 3. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)

    # 4. è½¬æ¢ Dataset
    test_dataset = Dataset.from_pandas(test_df)
    tokenized_test = test_dataset.map(
        lambda x: preprocess_test_function(x, tokenizer), 
        batched=True
    )

    # 5. ä½¿ç”¨ Trainer è¿›è¡Œé¢„æµ‹ï¼ˆè¿™æ ·å¤„ç† Batching å’Œ GPU æ¬è¿æœ€å¿«ï¼‰
    # æˆ‘ä»¬ä¸éœ€è¦è®­ç»ƒï¼Œæ‰€ä»¥å‚æ•°å¯ä»¥å¾ˆç®€å•
    predict_args = TrainingArguments(
        output_dir="./temp_preds",
        per_device_eval_batch_size=8, 
        fp16=True if torch.cuda.is_available() else False,
        dataloader_num_workers=0
    )

    trainer = Trainer(model=model, args=predict_args)

    print("æ­£åœ¨è¿›è¡Œé¢„æµ‹ï¼ˆInferenceï¼‰...")
    raw_preds = trainer.predict(tokenized_test)

    # 6. å°†é¢„æµ‹ç»“æœï¼ˆLogitsï¼‰è½¬åŒ–ä¸ºæ¦‚ç‡ï¼ˆSoftmaxï¼‰
    # æ¨¡å‹è¾“å‡ºçš„æ˜¯ä¸‰åˆ—æ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬å˜æˆåŠ èµ·æ¥ç­‰äº 1 çš„æ¦‚ç‡
    logits = torch.from_numpy(raw_preds.predictions)
    probs = torch.nn.functional.softmax(logits, dim=-1).numpy()

    # 7. ç”Ÿæˆæäº¤æ–‡ä»¶
    # å‡è®¾ï¼š0 -> model_a, 1 -> model_b, 2 -> tie
    submission = pd.DataFrame({
        'id': test_ids,
        'winner_model_a': probs[:, 0],
        'winner_model_b': probs[:, 1],
        'winner_tie': probs[:, 2]
    })

    submission.to_csv('submission.csv', index=False)
    print("ğŸ‰ é¢„æµ‹å®Œæˆï¼æäº¤æ–‡ä»¶ 'submission.csv' å·²ç”Ÿæˆã€‚")
